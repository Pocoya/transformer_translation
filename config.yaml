training:
  epochs: 20
  batch_size: 256
  learning_rate: 0.0004
  weight_decay: 0.1
  grad_clip: 1.0
  warmup_steps: 2000
  save_every: 2 
  label_smoothing: 0.15
  use_amp: true
  
model:
  src_vocab_size: 16000
  tgt_vocab_size: 16000
  d_model: 512
  n_heads: 8
  n_layers: 6
  dropout: 0.3
  d_ff: 2048
  
logging:
  log_every: 50
